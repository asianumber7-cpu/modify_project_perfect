import os
import logging
from typing import List, Optional

# ğŸš¨ FIX: langchain_communityê°€ ì•„ë‹Œ langchain_ibm ì‚¬ìš©
from langchain_ibm import WatsonxLLM
from langchain_huggingface import HuggingFaceEmbeddings

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# --- ì„¤ì • ---
EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
# LLM_MODEL_ID = "ibm/granite-13b-chat-v2" 
LLM_MODEL_ID = os.getenv("WATSONX_MODEL_ID", "ibm/granite-13b-chat-v2")

class ModelEngine:
    _instance: Optional['ModelEngine'] = None
    
    def __init__(self):
        self.text_llm: Optional[WatsonxLLM] = None
        self.embedding_model: Optional[HuggingFaceEmbeddings] = None
        self.is_initialized = False

    def initialize(self):
        """ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info(f"ğŸš€ Initializing Model Engine...")
        
        try:
            # 1. WatsonxLLM ì´ˆê¸°í™” (ì‹¤íŒ¨í•´ë„ ì„ë² ë”©ì€ ë¡œë“œ ì‹œë„í•˜ë„ë¡ try-except ë¶„ë¦¬)
            try:
                watsonx_api_key = os.getenv("WATSONX_API_KEY")
                project_id = os.getenv("WATSONX_PROJECT_ID")
                url = os.getenv("WATSONX_URL", "https://us-south.ml.cloud.ibm.com")

                if watsonx_api_key and project_id:
                    self.text_llm = WatsonxLLM(
                        model_id=LLM_MODEL_ID,
                        url=url,
                        apikey=watsonx_api_key,
                        project_id=project_id,
                        params={
                            "decoding_method": "greedy",
                            "max_new_tokens": 512,
                            "min_new_tokens": 1,
                            "temperature": 0.5
                        }
                    )
                    logger.info("âœ… Watsonx LLM Loaded.")
                else:
                    logger.warning("âš ï¸ Watsonx credentials not found. LLM disabled.")
            except Exception as e:
                logger.error(f"âŒ Watsonx LLM Init Failed: {e}")

            # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì—¬ê¸°ê°€ í•µì‹¬)
            logger.info(f"ğŸ“¥ Loading Embedding Model: {EMBEDDING_MODEL_NAME}...")
            self.embedding_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={'device': os.getenv("EMBEDDING_DEVICE", "cpu")},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("âœ… Embedding Model Loaded.")
            
            self.is_initialized = True
            
        except Exception as e:
            logger.error(f"âŒ Critical Error in Model Engine Init: {e}")
            # ì—¬ê¸°ì„œ ì—ëŸ¬ë¥¼ raise í•˜ì§€ ì•Šê³ , ê°œë³„ ë©”ì„œë“œì—ì„œ ì¬ì‹œë„í•˜ê²Œ í•¨

    def generate_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ -> ë²¡í„° ë³€í™˜ (ìë™ ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
        """
        # ğŸš¨ [Auto-Recovery] ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë”© ì‹œë„
        if not self.embedding_model:
            logger.warning("âš ï¸ Embedding model not ready. Attempting lazy load...")
            self.initialize()
            
        if not self.embedding_model:
             # ì¬ì‹œë„ í›„ì—ë„ ì—†ìœ¼ë©´ ì§„ì§œ ì—ëŸ¬
            raise RuntimeError("Embedding model is completely failed.")
            
        return self.embedding_model.embed_query(text)

    def generate_text(self, prompt: str) -> str:
        """
        LLM í…ìŠ¤íŠ¸ ìƒì„± (ìë™ ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
        """
        if not self.text_llm:
            logger.warning("âš ï¸ LLM not ready. Attempting lazy load...")
            self.initialize()
        
        if not self.text_llm:
            return "AI Model is not available."

        return self.text_llm.invoke(prompt)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
model_engine = ModelEngine()