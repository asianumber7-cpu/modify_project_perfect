import logging
import json
from fastapi import FastAPI, HTTPException, APIRouter, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

# model_engine (ì´ì „ê³¼ ë™ì¼)
from src.core.model_engine import model_engine

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- [LifeSpan] ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ Initializing AI Models...")
    try:
        model_engine.initialize()
    except Exception as e:
        logger.error(f"âš ï¸ Model init deferred: {e}")
    yield
    logger.info("ğŸ’¤ Shutting down...")

app = FastAPI(title="Modify AI Service", version="1.0.0", lifespan=lifespan)
api_router = APIRouter(prefix="/api/v1")

# --- [DTO ì •ì˜] ---
class EmbedRequest(BaseModel):
    text: str

class EmbedResponse(BaseModel):
    vector: List[float]

class ImageAnalysisResponse(BaseModel):
    name: str
    category: str
    description: str
    price: int
    vector: List[float]

# ğŸš¨ [NEW] ê²€ìƒ‰ìš© DTO ì¶”ê°€
class PathRequest(BaseModel):
    query: str

class InternalSearchRequest(BaseModel):
    query: str
    image_b64: Optional[str] = None  # ì´ë¯¸ì§€ê°€ ìˆì„ ê²½ìš° Base64ë¡œ ë°›ìŒ

class SearchProcessResponse(BaseModel):
    vector: List[float]
    reason: str

# --- [Existing Endpoints] ---

@api_router.post("/embed-text", response_model=EmbedResponse)
async def embed_text(request: EmbedRequest):
    """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì„ë² ë”© (ìƒí’ˆ ë“±ë¡ ì‹œ ì‚¬ìš©)"""
    try:
        vector = model_engine.generate_embedding(request.text)
        return {"vector": vector}
    except Exception as e:
        logger.error(f"Embedding Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/analyze-image", response_model=ImageAnalysisResponse)
async def analyze_image(file: UploadFile = File(...)):
    """ì´ë¯¸ì§€ ë¶„ì„ ë° JSON ìƒì„± (ìƒí’ˆ ë“±ë¡ ì‹œ ì‚¬ìš©)"""
    try:
        filename = file.filename
        prompt = f"""
        You are a professional fashion MD.
        Based on the image filename '{filename}', predict the product details.
        RULE: Return ONLY a JSON object with keys: "name", "category", "description", "price".
        """
        generated_text = model_engine.generate_text(prompt)
        
        try:
            cleaned_text = generated_text.replace("```json", "").replace("```", "").strip()
            product_data = json.loads(cleaned_text)
        except Exception:
            product_data = {
                "name": f"AI ë¶„ì„ ìƒí’ˆ ({filename})", 
                "category": "Uncategorized", 
                "description": "AI ë¶„ì„ ì‹¤íŒ¨", 
                "price": 0
            }

        meta_text = f"{product_data.get('name')} {product_data.get('category')} {product_data.get('description')}"
        vector = model_engine.generate_embedding(meta_text)

        return {
            "name": product_data.get("name"),
            "category": product_data.get("category"),
            "description": product_data.get("description"),
            "price": int(product_data.get("price", 0)),
            "vector": vector
        }
    except Exception as e:
        logger.error(f"Analysis Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- [NEW Endpoints for Search] ---
# Backendì˜ search.pyê°€ í˜¸ì¶œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë“¤ì…ë‹ˆë‹¤.

@api_router.post("/determine-path")
async def determine_path(request: PathRequest):
    """
    ê²€ìƒ‰ì–´ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ INTERNAL(ë‚´ë¶€ DB) ë˜ëŠ” EXTERNAL(ì™¸ë¶€ ê²€ìƒ‰) ê²½ë¡œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    try:
        # LLMì—ê²Œ íŒë‹¨ì„ ë§¡ê¸°ê±°ë‚˜, ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
        # ì‡¼í•‘ëª° ê²€ìƒ‰ì´ë¯€ë¡œ ê¸°ë³¸ê°’ì€ INTERNAL
        path = "INTERNAL"
        
        # (ì„ íƒì‚¬í•­) LLMì„ ì‚¬ìš©í•˜ì—¬ ì˜ë„ íŒŒì•…
        # intent_prompt = f"Is the query '{request.query}' asking for general news/trends (EXTERNAL) or searching for a product to buy (INTERNAL)? Reply only INTERNAL or EXTERNAL."
        # path = model_engine.generate_text(intent_prompt).strip()
        
        return {"path": path}
    except Exception as e:
        logger.error(f"Path determination failed: {e}")
        return {"path": "INTERNAL"} # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ ë‚´ë¶€ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬

@api_router.post("/process-internal", response_model=SearchProcessResponse)
async def process_internal(request: InternalSearchRequest):
    """
    ë‚´ë¶€ DB ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°ì™€ AI ì¶”ì²œ ë©˜íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        query = request.query
        
        # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ë²¡í„°í™” (ê°€ì¥ ì¤‘ìš”)
        vector = model_engine.generate_embedding(query)
        
        # 2. AI ì¶”ì²œ ë©˜íŠ¸ ìƒì„± (Watsonx í™œìš©)
        prompt = f"""
        ì‚¬ìš©ìê°€ ì‡¼í•‘ëª°ì—ì„œ '{query}'ë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤. 
        ì´ ê³ ê°ì—ê²Œ ë³´ì—¬ì¤„ ë§¤ë ¥ì ì¸ ìƒí’ˆ ì¶”ì²œ ë©˜íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        ì˜ˆì‹œ: "{query}ì™€ ê´€ë ¨ëœ íŠ¸ë Œë””í•œ ìƒí’ˆë“¤ì„ ëª¨ì•„ë´¤ìŠµë‹ˆë‹¤."
        """
        try:
            reason = model_engine.generate_text(prompt).strip()
        except Exception:
            reason = f"'{query}'ì— ëŒ€í•œ AI ì¶”ì²œ ê²°ê³¼ì…ë‹ˆë‹¤."
            
        return {"vector": vector, "reason": reason}

    except Exception as e:
        logger.error(f"Internal Search Process Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/process-external", response_model=SearchProcessResponse)
async def process_external(request: InternalSearchRequest):
    """
    ì™¸ë¶€ ê²€ìƒ‰ ì²˜ë¦¬ (í˜„ì¬ëŠ” ë‚´ë¶€ ê²€ìƒ‰ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë”ë¯¸ ë°ì´í„° ë°˜í™˜)
    """
    # í˜„ì¬ ì™¸ë¶€ ê²€ìƒ‰ ë¡œì§ì´ ì—†ìœ¼ë¯€ë¡œ ë‚´ë¶€ ê²€ìƒ‰ ë¡œì§ ì¬ì‚¬ìš©
    return await process_internal(request)


# Router ë“±ë¡
app.include_router(api_router)

@app.get("/")
def read_root():
    return {"message": "Modify AI Service is Running"}